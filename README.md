# Our solution to LLMs4Subjects (SemEval'25 task #5) ğŸš€
This repo contains both files from LLMs4Subjects' original repository and our files ğŸ“‚

## Authors
- [Vincenzo Avantaggiato](https://github.com/VincenzoAvantaggiato)
- [Michele Cazzola](https://github.com/MicheleCazzola)
- [Andrea Delli](https://github.com/RonPlusSign)

## General information
**Course**: `Large Language Models` (`Polytechnic of Turin`)  
**Academic Year**: 2024-25. Developed between January and February 2025.  
**Teachers**: Flavio Giobergia, Riccardo Coppola.  
**Topic**: development of a LLM-based method to perform authomatic subject tagging of technical documents from Leibniz Universityâ€™s Technical Library (TIBKAT).  

## Our approach
We explore three different approaches of increasing complexity:

1. **Embedding-based retrieval**: Generate embeddings for both documents and tags using an encoder LLM, compute cosine similarity, and assign the top-k highest scoring tags. ğŸ§©

2. **Fine-tuned embedding model**: Fine-tune a transformer-based encoder and apply the previous embedding-based retrieval approach. ğŸ”§

3. **Binary classification model**: Train a binary MLP that, given a document and a tag, predicts a similarity score, then selects the top-k highest scoring tags. ğŸ§ 

## Repository Structure ğŸ—‚ï¸
In the following, we'll report only the files we created/modified:

```
â”œâ”€â”€ images/                          # Directory containing images used in the project ğŸ–¼ï¸
â”œâ”€â”€ results/                         # Directory where results are stored ğŸ“Š
â”œâ”€â”€ main.ipynb                       # Main Jupyter notebook for running experiments ğŸ““
â”œâ”€â”€ embedding_similarity_tagging.py  # Script for tagging using embedding similarity ğŸ·ï¸
â”œâ”€â”€ finetune_sentence_transformer.py # Script for fine-tuning a sentence transformer model ğŸ”§
â”œâ”€â”€ binary_classifier.py             # Script for defining the multi-layer perceptron ğŸ§ 
â”œâ”€â”€ binary_mlp.py                    # Script for training the multi-layer perceptron ğŸ‹ï¸
â”œâ”€â”€ performances.py                  # Script for evaluating model performances ğŸ“ˆ
â”œâ”€â”€ plots.py                         # Script for generating plots ğŸ“‰
â”œâ”€â”€ README.md                        # Project documentation ğŸ“ƒ
â”œâ”€â”€ requirements.txt                 # List of dependencies required for the project ğŸ“‹
```

## Best results obtained ğŸ†
The best results are obtained by fine-tuning for three epochs using MultipleNegativesRankingLoss ğŸ”„
| k  | Precision (%) | Recall (%) | F1 Score (%) |
|----|---------------|------------|--------------|
| 5  | 9.74          | 21.70      | 13.28        |
| 10 | 6.30          | 27.12      | 10.14        |
| 15 | 4.73          | 29.75      | 8.11         |
| 20 | 3.87          | 32.16      | 6.88         |
| 25 | 3.30          | 33.89      | 5.98         |
| 30 | 2.91          | 35.68      | 5.36         |
| 35 | 2.61          | 36.99      | 4.86         |
| 40 | 2.36          | 37.83      | 4.43         |
| 45 | 2.16          | 38.92      | 4.08         |
| 50 | 1.99          | 39.76      | 3.87         |

# -*- coding: utf-8 -*-
"""llm_binary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1clme9RV7-CJBITYrTYgagjzqgCDGOpqr
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

# If you run this notebook in Google Colab, run this

# Clone repository and move its content in the current directory
!git clone https://github.com/RonPlusSign/llms4subjects.git
!mv llms4subjects/* .
!rm -r llms4subjects

# Install required packages
!pip install -r requirements.txt

"""## Model"""

class BinaryClassifier(nn.Module):
    def __init__(self, in_features, hidden_dims):
        super(BinaryClassifier, self).__init__()

        self.in_features = in_features
        self.hidden_dims = hidden_dims

        self.input = nn.Sequential(
            nn.Linear(in_features, hidden_dims[0]),
            nn.ReLU()
        )

        layers_list = [
            nn.Sequential(
                nn.Linear(prev, curr),
                nn.ReLU()
            )
            for prev, curr in zip(hidden_dims[:-1], hidden_dims[1:])
        ]

        self.layers = nn.Sequential(*layers_list)
        self.output = nn.Linear(hidden_dims[-1], 1)

    def forward(self, x):
        out = self.input(x)
        out = self.layers(out)
        out = self.output(out)

        return out

"""## Evaluate function"""

def evaluate(
        model: nn.Module,
        dataloader: DataLoader,
        criterion,
        device: torch.device,
        log_frequency: int
):
    model.eval()
    steps, iterations = 0, 0
    eval_loss = 0

    for (text, tag, label) in dataloader:
        input = torch.hstack((text, tag)).to(device)
        label = label.long().to(device)

        pred = model(input)
        loss = criterion(pred, label)

        eval_loss += loss.item()

        if (steps + 1) % log_frequency == 0:
            print(f"Iteration {steps}, Loss: {eval_loss / (steps + 1)}")

        steps += 1

    return eval_loss / len(dataloader)

"""## Train function"""

def train(
        model: nn.Module,
        trainloader: DataLoader,
        validloader: DataLoader,
        num_epochs: int,
        criterion,
        optimizer,
        device: torch.device,
        train_log_frequency: int,
        val_log_frequency: int,
        eval_epoch_frequency: int
):

    train_losses = []
    val_losses = []
    for epoch in len(range(num_epochs)):
        model.train()
        steps = 0
        train_loss = 0
        for (text, tag, label) in trainloader:
            input = torch.hstack((text, tag)).to(device)
            label[label < 0] = 0
            label = (label).long().to(device)

            pred = model(input)
            loss = criterion(pred, label)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()

            if (steps + 1) % train_log_frequency == 0:
                print(f"Epoch {epoch+1}, Iteration {steps}, Loss: {train_loss / (steps + 1)}")

            steps += 1

        train_loss /= len(trainloader)
        train_losses.append(train_loss)

        if (epoch + 1) % eval_epoch_frequency == 0:
            val_loss = evaluate(model, validloader, criterion, val_log_frequency)
            val_losses.append(val_loss)

        if (epoch + 1) % eval_epoch_frequency == 0:
            print(f"End of epoch {epoch + 1}, Training loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}")
        else:
            print(f"End of epoch {epoch + 1}, Training loss: {train_losses[-1]}")

    return train_losses, val_losses

"""## Test snippet"""

EMBEDDING_DIM = 768
BATCH_SIZE = 10
IN_FEATURES = EMBEDDING_DIM * 2


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

x = torch.randn((BATCH_SIZE, IN_FEATURES))

hidden_dimensions = [2 * IN_FEATURES, 2048, 1024]
model = BinaryClassifier(IN_FEATURES, hidden_dimensions)

num_params = sum([p.numel() for p in model.parameters()])

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

model.to(device)
x = x.to(device)

out = model(x)
loss_0 = criterion(out, torch.ones((BATCH_SIZE, 1)).to(device))
loss_1 = criterion(out, torch.zeros((BATCH_SIZE, 1)).to(device))

print(loss_0, loss_1)
print(out, torch.nn.Sigmoid()(out))

"""## Resource utilization

### Latency
"""

STEPS = 100_000
x = torch.randn((1, IN_FEATURES)).to(device)

start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)

start_event.record()
for _ in range(STEPS):
    _ = model(x)
end_event.record()

torch.cuda.synchronize()

latency_ms = start_event.elapsed_time(end_event) / STEPS

print(latency_ms)

"""### Params"""

num_params = sum([p.numel() for p in model.parameters()])
print(num_params)

"""### FLOPS"""

!pip install fvcore

from fvcore.nn import FlopCountAnalysis

flops = FlopCountAnalysis(model, x)
print(flops.total())
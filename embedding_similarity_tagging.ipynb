{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ed7fb34b9485ee",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RonPlusSign/llms4subjects/blob/main/embedding_similarity_tagging.ipynb)\n",
    "\n",
    "# Embedding Similarity Tagging\n",
    "\n",
    "The goal of this notebook is to run the `embedding_similarity_tagging.py` script with different parameters (e.g. different embedding models).\n",
    "\n",
    "The script uses a SentenceTransformer model to encode document texts and tag embeddings,\n",
    "and then computes the similarity between them to tag the documents with the most similar GND tags.\n",
    "\n",
    "The quality of the tagging results is evaluated using the `shared-task-eval-script/llms4subjects-evaluation.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f85a4a4082790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook in Google Colab, run this\n",
    "\n",
    "# Clone repository and move its content in the current directory\n",
    "!git clone https://github.com/RonPlusSign/llms4subjects.git\n",
    "!mv llms4subjects/* .\n",
    "!rm -r llms4subjects\n",
    "\n",
    "# Install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e291c6cdff596",
   "metadata": {},
   "source": [
    "#### Tagging with Different Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ddebb865d09575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T16:43:19.441800Z",
     "start_time": "2025-02-18T16:43:19.437462Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"distiluse-base-multilingual-cased-v1\",\n",
    "    \"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\",  # this gives warning \"No sentence-transformers model found with name ...\", but it's ok\n",
    "    \"intfloat/multilingual-e5-large\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa95abf7cda27ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T18:04:41.818900Z",
     "start_time": "2025-02-18T16:43:29.530764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Running tagging with model: sentence-transformers/all-MiniLM-L6-v2 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\andre\\Desktop\\llms4subjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:17<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:31<00:00, 46.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: distiluse-base-multilingual-cased-v1 ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [04:22<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:41<00:00, 43.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: T-Systems-onsite/cross-en-de-roberta-sentence-transformer ------\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name T-Systems-onsite/cross-en-de-roberta-sentence-transformer. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [09:01<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [07:12<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: intfloat/multilingual-e5-large ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [25:40<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [20:48<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    model_name_folder = model_name.split(\"/\")[-1]\n",
    "    tag_embeddings_file = f\"results/{model_name_folder}/tag_embeddings.json\"  # Where to save the tag embeddings\n",
    "    results_dir = f\"results/{model_name_folder}\"  # Where to save the tagging results\n",
    "    docs_path = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"  # Documents to tag\n",
    "    tag_file = \"shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json\"  # Tag list definition\n",
    "\n",
    "    print(f\"\\n------Running tagging with model: {model_name} ------\")\n",
    "    %run embedding_similarity_tagging.py \\\n",
    "            --model_name { model_name } \\\n",
    "            --tags_file { tag_file } \\\n",
    "            --tag_embeddings_file { tag_embeddings_file } \\\n",
    "            --results_dir { results_dir } \\\n",
    "            --docs_path { docs_path }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a565e13ca69209",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f0e5c01e5f754d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T18:08:57.481245Z",
     "start_time": "2025-02-18T18:04:41.954643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Evaluating tagging results for model: sentence-transformers/all-MiniLM-L6-v2 ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/all-MiniLM-L6-v2/all-MiniLM-L6-v2_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating tagging results for model: distiluse-base-multilingual-cased-v1 ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/distiluse-base-multilingual-cased-v1/distiluse-base-multilingual-cased-v1_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating tagging results for model: T-Systems-onsite/cross-en-de-roberta-sentence-transformer ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/cross-en-de-roberta-sentence-transformer/cross-en-de-roberta-sentence-transformer_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating tagging results for model: intfloat/multilingual-e5-large ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/multilingual-e5-large/multilingual-e5-large_evaluation_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tagging results using the evaluation script.\n",
    "for model_name in models:\n",
    "    print(f\"\\n------Evaluating tagging results for model: {model_name} ------\")\n",
    "\n",
    "    model_name_folder = model_name.split(\"/\")[-1]\n",
    "    true_labels_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"\n",
    "    pred_labels_dir = f\"results/{model_name_folder}\"\n",
    "    results_dir = f\"results/{model_name_folder}\"\n",
    "\n",
    "    %run \"shared-task-eval-script/llms4subjects-evaluation.py\" \\\n",
    "            --team_name { model_name_folder } \\\n",
    "            --true_labels_dir { true_labels_dir } \\\n",
    "            --pred_labels_dir { pred_labels_dir } \\\n",
    "            --results_dir { results_dir }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b55bd4ce42a57",
   "metadata": {},
   "source": [
    "## SentenceTransformer fine-tuning\n",
    "\n",
    "The `finetune_sentence_transformer.py` script fine-tunes a SentenceTransformer model on training data for subject tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d183ab03cb8e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T18:38:53.855846Z",
     "start_time": "2025-02-19T18:38:53.851714Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of models to fine-tune\n",
    "models = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # \"distiluse-base-multilingual-cased-v1\",\n",
    "    # \"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\", # this gives warning \"No sentence-transformers model found with name ...\", but it's ok\n",
    "    # \"intfloat/multilingual-e5-large\",\n",
    "]\n",
    "\n",
    "# Extract the base model names (e.g., from \"sentence-transformers/all-MiniLM-L6-v2\" take \"all-MiniLM-L6-v2\")\n",
    "model_names = [model_name.split(\"/\")[-1] for model_name in models]\n",
    "\n",
    "# Define the list of losses to test\n",
    "losses = [\"coSENT\", \"AnglE\", \"CosineSimilarity\", \"MultipleNegativesRanking\", \"Triplet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080c1a767866eef",
   "metadata": {},
   "source": [
    "#### Execute fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e1194441507d78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:56:23.722967Z",
     "start_time": "2025-02-19T11:47:30.049198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Fine-tuning model: sentence-transformers/all-MiniLM-L6-v2 ------\n",
      "Using loss: coSENT, saving to: models/finetuned/all-MiniLM-L6-v2_coSENT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\andre\\Desktop\\llms4subjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model...\n",
      "Loading GND tags and building mapping...\n",
      "Loaded 79427 GND tags.\n",
      "Building training examples...\n",
      "Created 87896 training examples.\n",
      "Building evaluation examples...\n",
      "Created 14711 evaluation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: andrea-delli (andrea-delli-politecnico-di-torino) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\andre\\Desktop\\llms4subjects\\wandb\\run-20250219_124751-wvjx0rjy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/wvjx0rjy' target=\"_blank\">finetune_all-MiniLM-L6-v2_coSENTLoss</a></strong> to <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/wvjx0rjy' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/wvjx0rjy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10987' max='10987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10987/10987 1:27:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.408500</td>\n",
       "      <td>0.669412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.927000</td>\n",
       "      <td>0.588041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.729700</td>\n",
       "      <td>0.451133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0.460791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.260500</td>\n",
       "      <td>0.368131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.298800</td>\n",
       "      <td>0.399303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.173900</td>\n",
       "      <td>0.355184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.002100</td>\n",
       "      <td>0.311243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>0.293465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.021500</td>\n",
       "      <td>0.274140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved to models/finetuned/all-MiniLM-L6-v2_coSENT.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▄▄▃▃▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▁▅█▅▅▆▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▇██▄▁▄▃▃▇█</td></tr><tr><td>eval/steps_per_second</td><td>▇██▄▁▄▃▃▇█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>▂▂▇▂▅▁▃▁█▁</td></tr><tr><td>train/learning_rate</td><td>██▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.27414</td></tr><tr><td>eval/runtime</td><td>173.9312</td></tr><tr><td>eval/samples_per_second</td><td>169.159</td></tr><tr><td>eval/steps_per_second</td><td>21.146</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>10987</td></tr><tr><td>train/grad_norm</td><td>0.00283</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0215</td></tr><tr><td>train_loss</td><td>1.39088</td></tr><tr><td>train_runtime</td><td>5271.7505</td></tr><tr><td>train_samples_per_second</td><td>33.346</td></tr><tr><td>train_steps_per_second</td><td>2.084</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finetune_all-MiniLM-L6-v2_coSENTLoss</strong> at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/wvjx0rjy' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/wvjx0rjy</a><br> View project at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250219_124751-wvjx0rjy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss: AnglE, saving to: models/finetuned/all-MiniLM-L6-v2_AnglE\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "Loading GND tags and building mapping...\n",
      "Loaded 79427 GND tags.\n",
      "Building training examples...\n",
      "Created 87896 training examples.\n",
      "Building evaluation examples...\n",
      "Created 14711 evaluation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\andre\\Desktop\\llms4subjects\\wandb\\run-20250219_141601-rgo6xajy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/rgo6xajy' target=\"_blank\">finetune_all-MiniLM-L6-v2_AnglELoss</a></strong> to <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/rgo6xajy' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/rgo6xajy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10987' max='10987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10987/10987 1:33:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.708900</td>\n",
       "      <td>0.726152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.000800</td>\n",
       "      <td>0.685210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.825400</td>\n",
       "      <td>0.507084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.544500</td>\n",
       "      <td>0.505828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.346300</td>\n",
       "      <td>0.416654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.377200</td>\n",
       "      <td>0.419160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.252200</td>\n",
       "      <td>0.369957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.052100</td>\n",
       "      <td>0.352722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.054700</td>\n",
       "      <td>0.339808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>0.306802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved to models/finetuned/all-MiniLM-L6-v2_AnglE.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▄▄▃▃▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▄█▃▃▃▂▂▁▄▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▁▆▆▆▇▇█▅▇</td></tr><tr><td>eval/steps_per_second</td><td>▅▁▆▆▆▇▇█▅▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>▅▄▃▂▆▁▇▁█▁</td></tr><tr><td>train/learning_rate</td><td>██▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.3068</td></tr><tr><td>eval/runtime</td><td>193.2063</td></tr><tr><td>eval/samples_per_second</td><td>152.283</td></tr><tr><td>eval/steps_per_second</td><td>19.037</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>10987</td></tr><tr><td>train/grad_norm</td><td>0.0228</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0456</td></tr><tr><td>train_loss</td><td>1.47274</td></tr><tr><td>train_runtime</td><td>5594.9124</td></tr><tr><td>train_samples_per_second</td><td>31.42</td></tr><tr><td>train_steps_per_second</td><td>1.964</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finetune_all-MiniLM-L6-v2_AnglELoss</strong> at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/rgo6xajy' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/rgo6xajy</a><br> View project at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250219_141601-rgo6xajy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss: CosineSimilarity, saving to: models/finetuned/all-MiniLM-L6-v2_CosineSimilarity\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "Loading GND tags and building mapping...\n",
      "Loaded 79427 GND tags.\n",
      "Building training examples...\n",
      "Created 87896 training examples.\n",
      "Building evaluation examples...\n",
      "Created 14711 evaluation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\andre\\Desktop\\llms4subjects\\wandb\\run-20250219_154931-0aat84zo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/0aat84zo' target=\"_blank\">finetune_all-MiniLM-L6-v2_CosineSimilarityLoss</a></strong> to <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/0aat84zo' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/0aat84zo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10987' max='10987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10987/10987 1:35:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.083757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.067603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.064060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.059908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.058677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.056789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.053862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.052989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.050934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.050552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved to models/finetuned/all-MiniLM-L6-v2_CosineSimilarity.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁█▇▇▇▇▆▃</td></tr><tr><td>eval/samples_per_second</td><td>███▁▂▂▂▂▃▆</td></tr><tr><td>eval/steps_per_second</td><td>███▁▂▂▂▂▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>▅▄▄▄█▁▅▂▆▁</td></tr><tr><td>train/learning_rate</td><td>██▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.05055</td></tr><tr><td>eval/runtime</td><td>205.5515</td></tr><tr><td>eval/samples_per_second</td><td>143.137</td></tr><tr><td>eval/steps_per_second</td><td>17.893</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>10987</td></tr><tr><td>train/grad_norm</td><td>0.30053</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0526</td></tr><tr><td>train_loss</td><td>0.06537</td></tr><tr><td>train_runtime</td><td>5714.8332</td></tr><tr><td>train_samples_per_second</td><td>30.761</td></tr><tr><td>train_steps_per_second</td><td>1.923</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finetune_all-MiniLM-L6-v2_CosineSimilarityLoss</strong> at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/0aat84zo' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/0aat84zo</a><br> View project at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250219_154931-0aat84zo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss: MultipleNegativesRanking, saving to: models/finetuned/all-MiniLM-L6-v2_MultipleNegativesRanking\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "Loading GND tags and building mapping...\n",
      "Loaded 79427 GND tags.\n",
      "Building training examples...\n",
      "Created 87896 training examples.\n",
      "Building evaluation examples...\n",
      "Created 14711 evaluation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\andre\\Desktop\\llms4subjects\\wandb\\run-20250219_172501-zsfmnwq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/zsfmnwq9' target=\"_blank\">finetune_all-MiniLM-L6-v2_MultipleNegativesRankingLoss</a></strong> to <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/zsfmnwq9' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/zsfmnwq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5494' max='5494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5494/5494 40:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.407056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>0.335982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.302286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.279303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.264122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved to models/finetuned/all-MiniLM-L6-v2_MultipleNegativesRanking.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>▆▁▃█▆</td></tr><tr><td>eval/samples_per_second</td><td>▃█▆▁▃</td></tr><tr><td>eval/steps_per_second</td><td>▃█▆▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▄▄▆▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▄▄▆▆▇▇█</td></tr><tr><td>train/grad_norm</td><td>▇▆▅█▁</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.26412</td></tr><tr><td>eval/runtime</td><td>108.057</td></tr><tr><td>eval/samples_per_second</td><td>136.141</td></tr><tr><td>eval/steps_per_second</td><td>17.019</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5494</td></tr><tr><td>train/grad_norm</td><td>8.43434</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4813</td></tr><tr><td>train_loss</td><td>0.60807</td></tr><tr><td>train_runtime</td><td>2420.1779</td></tr><tr><td>train_samples_per_second</td><td>36.318</td></tr><tr><td>train_steps_per_second</td><td>2.27</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finetune_all-MiniLM-L6-v2_MultipleNegativesRankingLoss</strong> at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/zsfmnwq9' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/zsfmnwq9</a><br> View project at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250219_172501-zsfmnwq9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss: Triplet, saving to: models/finetuned/all-MiniLM-L6-v2_Triplet\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "Loading GND tags and building mapping...\n",
      "Loaded 79427 GND tags.\n",
      "Building training examples...\n",
      "Created 87896 training examples.\n",
      "Building evaluation examples...\n",
      "Created 14711 evaluation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\andre\\Desktop\\llms4subjects\\wandb\\run-20250219_180534-6qanqd1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/6qanqd1a' target=\"_blank\">finetune_all-MiniLM-L6-v2_TripletLoss</a></strong> to <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/6qanqd1a' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/6qanqd1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5494' max='5494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5494/5494 50:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.146700</td>\n",
       "      <td>3.918621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.855800</td>\n",
       "      <td>3.748374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.779700</td>\n",
       "      <td>3.729603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.750900</td>\n",
       "      <td>3.691578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.721100</td>\n",
       "      <td>3.691401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved to models/finetuned/all-MiniLM-L6-v2_Triplet.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▇██▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇██▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▄▄▆▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▄▄▆▆▇▇█</td></tr><tr><td>train/grad_norm</td><td>▃▁█▁▁</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.6914</td></tr><tr><td>eval/runtime</td><td>136.7844</td></tr><tr><td>eval/samples_per_second</td><td>107.549</td></tr><tr><td>eval/steps_per_second</td><td>13.445</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5494</td></tr><tr><td>train/grad_norm</td><td>0.71183</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.7211</td></tr><tr><td>train_loss</td><td>3.83536</td></tr><tr><td>train_runtime</td><td>3046.686</td></tr><tr><td>train_samples_per_second</td><td>28.85</td></tr><tr><td>train_steps_per_second</td><td>1.803</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">finetune_all-MiniLM-L6-v2_TripletLoss</strong> at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/6qanqd1a' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers/runs/6qanqd1a</a><br> View project at: <a href='https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers' target=\"_blank\">https://wandb.ai/andrea-delli-politecnico-di-torino/sentence-transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250219_180534-6qanqd1a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetune all SentenceTransformer models on the training data for each loss function\n",
    "for model_name in models:\n",
    "    print(f\"\\n------Fine-tuning model: {model_name} ------\")\n",
    "    model_name_clean = model_name.split(\"/\")[-1]\n",
    "\n",
    "    # Common directories and files\n",
    "    training_data_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/train\"\n",
    "    eval_data_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"\n",
    "    gnd_tags_file = \"shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json\"\n",
    "\n",
    "    for loss in losses:\n",
    "        # Modify the output path to include the loss function name\n",
    "        output_model_path = f\"models/finetuned/{model_name_clean}_{loss}\"\n",
    "        print(f\"Using loss: {loss}, saving to: {output_model_path}\")\n",
    "\n",
    "        # Run the fine-tuning script with the specified loss\n",
    "        %run finetune_sentence_transformer.py \\\n",
    "                --training_path { training_data_dir } \\\n",
    "                --eval_path { eval_data_dir } \\\n",
    "                --gnd_tags_file { gnd_tags_file } \\\n",
    "                --model_name { model_name } \\\n",
    "                --output_model_path { output_model_path } \\\n",
    "                --batch_size 16 \\\n",
    "                --num_epochs 1 \\\n",
    "                --loss { loss }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7622723d1cf5603",
   "metadata": {},
   "source": [
    "#### Tag using the fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce2aa87811249f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:04:17.469114Z",
     "start_time": "2025-02-19T18:38:58.923892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Running tagging with model: models/finetuned/all-MiniLM-L6-v2_coSENT (loss: coSENT) ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\andre\\Desktop\\llms4subjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:16<00:00, 32.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [03:11<00:00, 36.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: models/finetuned/all-MiniLM-L6-v2_AnglE (loss: AnglE) ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:17<00:00, 31.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:54<00:00, 40.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: models/finetuned/all-MiniLM-L6-v2_CosineSimilarity (loss: CosineSimilarity) ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:16<00:00, 32.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:48<00:00, 41.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: models/finetuned/all-MiniLM-L6-v2_MultipleNegativesRanking (loss: MultipleNegativesRanking) ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:15<00:00, 32.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:16<00:00, 50.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n",
      "\n",
      "------Running tagging with model: models/finetuned/all-MiniLM-L6-v2_Triplet (loss: Triplet) ------\n",
      "Loading model...\n",
      "Loading GND tags...\n",
      "Encoding tag descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2483/2483 [01:15<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test documents and computing similarities...\n",
      "Found 6980 documents in shared-task-datasets/TIBKAT/tib-core-subjects/data/dev.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging documents: 100%|██████████| 6980/6980 [02:17<00:00, 50.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Individual results saved in corresponding files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For each model and each loss, run tagging\n",
    "for model_name in model_names:\n",
    "    for loss in losses:\n",
    "        # Construct the path where the finetuned model is saved\n",
    "        finetuned_model_path = f\"models/finetuned/{model_name}_{loss}\"\n",
    "        # Use a folder name that includes both the model and loss to save the tagging results\n",
    "        model_name_folder = f\"{model_name}_{loss}\"\n",
    "        tag_embeddings_file = f\"results/finetuned_{model_name_folder}/tag_embeddings.json\"  # Where to save the tag embeddings\n",
    "        results_dir = f\"results/finetuned_{model_name_folder}\"  # Where to save the tagging results\n",
    "        docs_path = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"  # Documents to tag\n",
    "        tag_file = \"shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json\"  # Tag list definition\n",
    "\n",
    "        print(f\"\\n------Running tagging with model: {finetuned_model_path} (loss: {loss}) ------\")\n",
    "        %run embedding_similarity_tagging.py \\\n",
    "                --model_name { finetuned_model_path } \\\n",
    "                --tags_file { tag_file } \\\n",
    "                --tag_embeddings_file { tag_embeddings_file } \\\n",
    "                --results_dir { results_dir } \\\n",
    "                --docs_path { docs_path }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c30c1b5024d589",
   "metadata": {},
   "source": [
    "#### Evaluate the fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1a313a4b87661b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:07:18.777133Z",
     "start_time": "2025-02-19T19:04:34.406821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Evaluating fine-tuned model: models/finetuned/all-MiniLM-L6-v2_coSENT (loss: coSENT) ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/finetuned_all-MiniLM-L6-v2_coSENT/finetuned_all-MiniLM-L6-v2_coSENT_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating fine-tuned model: models/finetuned/all-MiniLM-L6-v2_AnglE (loss: AnglE) ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/finetuned_all-MiniLM-L6-v2_AnglE/finetuned_all-MiniLM-L6-v2_AnglE_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating fine-tuned model: models/finetuned/all-MiniLM-L6-v2_CosineSimilarity (loss: CosineSimilarity) ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/finetuned_all-MiniLM-L6-v2_CosineSimilarity/finetuned_all-MiniLM-L6-v2_CosineSimilarity_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating fine-tuned model: models/finetuned/all-MiniLM-L6-v2_MultipleNegativesRanking (loss: MultipleNegativesRanking) ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/finetuned_all-MiniLM-L6-v2_MultipleNegativesRanking/finetuned_all-MiniLM-L6-v2_MultipleNegativesRanking_evaluation_metrics.xlsx\n",
      "\n",
      "------Evaluating fine-tuned model: models/finetuned/all-MiniLM-L6-v2_Triplet (loss: Triplet) ------\n",
      "\n",
      "LLMs4Subjects Shared Task -- Evaluations\n",
      "\n",
      "Reading the True GND labels...\n",
      "Reading the Predicted GND labels...\n",
      "\n",
      "Evaluating the directory structure of the predicted folder...\n",
      "\n",
      "Evaluating the predicted GND labels...\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 5\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 5\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 10\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 10\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 15\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 15\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 20\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 20\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 25\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 25\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 30\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 30\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 35\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 35\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 40\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 40\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 45\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 45\n",
      "\n",
      "Evaluating GND Subject Codes -- Granularity Level: Combined Language and Record-levels and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Record Type level and k: 50\n",
      "Evaluating GND Subject Codes -- Granularity Level: Language level and k: 50\n",
      "\n",
      "File containing the evaluation metrics score is saved at location: results/finetuned_all-MiniLM-L6-v2_Triplet/finetuned_all-MiniLM-L6-v2_Triplet_evaluation_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each fine-tuned models (i.e. for each model-loss combination)\n",
    "for model_name in model_names:\n",
    "    for loss in losses:\n",
    "        # Build the fine-tuned model path used during training\n",
    "        finetuned_model_path = f\"models/finetuned/{model_name}_{loss}\"\n",
    "        # Create a unique folder name that includes both model name and loss\n",
    "        model_name_clean = f\"{model_name}_{loss}\"\n",
    "        true_labels_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"\n",
    "        pred_labels_dir = f\"results/finetuned_{model_name_clean}\"\n",
    "        results_dir = f\"results/finetuned_{model_name_clean}\"\n",
    "        result_name = f\"finetuned_{model_name_clean}\"\n",
    "\n",
    "        print(f\"\\n------Evaluating fine-tuned model: {finetuned_model_path} (loss: {loss}) ------\")\n",
    "        %run \"shared-task-eval-script/llms4subjects-evaluation.py\" \\\n",
    "                --team_name { result_name } \\\n",
    "                --true_labels_dir { true_labels_dir } \\\n",
    "                --pred_labels_dir { pred_labels_dir } \\\n",
    "                --results_dir { results_dir }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f47a6ac2e047a6",
   "metadata": {},
   "source": [
    "## Use MLP instead of cosine similarity for tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b0c9b8c1c1e3",
   "metadata": {},
   "source": [
    "#### Train the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38b318bfae7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "model_name_clean = model_name.split(\"/\")[-1]\n",
    "model_path = f\"{model_name_clean}\"\n",
    "training_data_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/train\"\n",
    "eval_data_dir = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"\n",
    "gnd_tags_file = \"shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json\"\n",
    "\n",
    "%run binary_mlp.py \\\n",
    "        --training_path {training_data_dir} \\\n",
    "        --eval_path {eval_data_dir} \\\n",
    "        --gnd_tags_file {gnd_tags_file} \\\n",
    "        --model_name {model_path} \\\n",
    "        --batch_size 16 \\\n",
    "        --num_epochs 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66763814b055468",
   "metadata": {},
   "source": [
    "#### Tag using the embedding models + MLP to measure similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580837ae6654b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_folder = model_name.split(\"/\")[-1]\n",
    "tag_embeddings_file = f\"results/{model_name_folder}/tag_embeddings.json\"  # Where to save the tag embeddings\n",
    "results_dir = f\"results/{model_name_folder}\"  # Where to save the tagging results\n",
    "docs_path = \"shared-task-datasets/TIBKAT/tib-core-subjects/data/dev\"  # Documents to tag\n",
    "tag_file = \"shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json\"  # Tag list definition\n",
    "mlp_model = f\"models/mlp/{model_name_folder}\"\n",
    "\n",
    "print(f\"\\n------Running tagging with model: {model_name} ------\")\n",
    "%run embedding_similarity_tagging.py \\\n",
    "        --model_name { model_name } \\\n",
    "        --tags_file { tag_file } \\\n",
    "        --tag_embeddings_file { tag_embeddings_file } \\\n",
    "        --results_dir { results_dir } \\\n",
    "        --docs_path { docs_path } \\\n",
    "        --mlp_model { mlp_model }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
